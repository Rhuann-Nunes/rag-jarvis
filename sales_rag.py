import os
from typing import List, Dict, Any, Optional
import logging
import pathlib
from datetime import datetime

from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from groq import Groq
from langchain_text_splitters import MarkdownTextSplitter

import config
from user_data_service import UserDataService

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Conte√∫do embutido do JARVIS para usar quando o arquivo n√£o estiver dispon√≠vel
JARVIS_SALES_CONTENT = """# JARVIS: Seu Assistente Pessoal de IA

## O que √© o JARVIS?

O JARVIS √© um assistente pessoal de produtividade alimentado por intelig√™ncia artificial, inspirado no famoso assistente do Homem de Ferro. Diferente de outros aplicativos de tarefas, o JARVIS entende linguagem natural e organiza sua vida atrav√©s de uma interface intuitiva e acess√≠vel de qualquer dispositivo.

**[‚û°Ô∏è Acesse agora o JARVIS: https://www.appjarvis.com.br/](https://www.appjarvis.com.br/)**

## Como o JARVIS transforma sua produtividade

### üîç Entendendo Linguagem Natural

O JARVIS utiliza modelos avan√ßados de IA (GPT-4o) para compreender suas tarefas exatamente como voc√™ as descreve.

### üìÖ Gest√£o Inteligente de Tarefas

O sistema organiza suas tarefas automaticamente com base em datas inteligentes, recorr√™ncias e categoriza√ß√£o.

### üîî Notifica√ß√µes Que Funcionam

Alertas no dispositivo, notifica√ß√µes via WhatsApp e prioriza√ß√£o inteligente.

### ü§ñ JARVIS no WhatsApp

Acesso total aos seus dados, assistente proativo, cria e gerencia tarefas.

## Seguran√ßa e Privacidade

Autentica√ß√£o segura, armazenamento criptografado e controle total dos dados.

## Comece Agora - Vagas Limitadas!

**[üöÄ Experimente o JARVIS GRATUITAMENTE: https://www.appjarvis.com.br/](https://www.appjarvis.com.br/)**

## Planos Acess√≠veis

- 7 Dias Gr√°tis ‚Äì Experimente todas as funcionalidades sem cart√£o de cr√©dito
- Plano Anual ‚Äì Apenas R$ 50,00 por um ano inteiro de produtividade transformadora"""


# M√≥dulo de gerenciamento de conte√∫do
class ContentManager:
    """Gerencia o carregamento e processamento do conte√∫do de vendas."""
    
    def __init__(self, content_text=None, chunk_size=1000, chunk_overlap=200):
        """
        Inicializa o gerenciador de conte√∫do.
        
        Args:
            content_text (str, optional): Texto do conte√∫do de vendas. Usa o conte√∫do embutido se None.
            chunk_size (int): Tamanho de cada fragmento para o text splitter.
            chunk_overlap (int): Sobreposi√ß√£o entre fragmentos para o text splitter.
        """
        self.content_text = content_text or JARVIS_SALES_CONTENT
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.text_splitter = MarkdownTextSplitter(
            chunk_size=self.chunk_size, 
            chunk_overlap=self.chunk_overlap
        )
        
    def load_content(self) -> List[Document]:
        """
        Carrega e processa o conte√∫do de vendas.
        
        Returns:
            List[Document]: Lista de documentos processados.
        """
        try:
            # Processar o conte√∫do como um documento
            document = Document(page_content=self.content_text)
            documents = [document]
            
            # Split the document into chunks
            chunks = self.text_splitter.split_documents(documents)
            
            # Add metadata to identify sections
            for i, doc in enumerate(chunks):
                # Extract section from content if possible
                content = doc.page_content
                section = "General"
                
                if "## " in content:
                    section_line = content.split("## ")[1].split("\n")[0]
                    section = section_line.strip()
                elif "### " in content:
                    section_line = content.split("### ")[1].split("\n")[0]
                    section = section_line.strip()
                
                doc.metadata["section"] = section
                doc.metadata["chunk_id"] = i
                doc.metadata["source"] = "jarvis_sales_content"
            
            logger.info(f"Processed content into {len(chunks)} chunks")
            return chunks
        except Exception as e:
            logger.error(f"Error processing content: {str(e)}", exc_info=True)
            # Return default content to avoid cascading errors
            default_doc = Document(
                page_content="JARVIS: Seu Assistente Pessoal de IA. O JARVIS √© um assistente pessoal de produtividade alimentado por intelig√™ncia artificial.",
                metadata={"section": "Geral", "chunk_id": 0, "source": "default_content"}
            )
            return [default_doc]


# M√≥dulo de embedding
class EmbeddingService:
    """Servi√ßo para gera√ß√£o e gest√£o de embeddings."""
    
    def __init__(self, embedding_model=None):
        """
        Inicializa o servi√ßo de embeddings.
        
        Args:
            embedding_model: Modelo de embeddings a ser usado. Se None, usa OpenAIEmbeddings padr√£o.
        """
        self.embedding_model = embedding_model or OpenAIEmbeddings(
            model=os.getenv("EMBEDDING_MODEL_NAME", "text-embedding-3-small")
        )
        # Cache para armazenar embeddings calculados
        self.embedding_cache = {}
        
    def get_embeddings_for_documents(self, documents: List[Document]) -> Dict[int, List[float]]:
        """
        Gera embeddings para uma lista de documentos.
        
        Args:
            documents (List[Document]): Lista de documentos para gerar embeddings.
            
        Returns:
            Dict[int, List[float]]: Dicion√°rio de embeddings por √≠ndice de documento.
        """
        embeddings = {}
        for i, doc in enumerate(documents):
            # Usar cache se dispon√≠vel
            cache_key = hash(doc.page_content)
            if cache_key in self.embedding_cache:
                embeddings[i] = self.embedding_cache[cache_key]
                continue
                
            # Calcular novo embedding
            embedding = self.embedding_model.embed_documents([doc.page_content])[0]
            self.embedding_cache[cache_key] = embedding
            embeddings[i] = embedding
            
        return embeddings
        
    def embed_query(self, query: str) -> List[float]:
        """
        Gera embedding para uma consulta.
        
        Args:
            query (str): Texto da consulta.
            
        Returns:
            List[float]: Embedding da consulta.
        """
        # Usar cache se dispon√≠vel
        cache_key = hash(query)
        if cache_key in self.embedding_cache:
            return self.embedding_cache[cache_key]
            
        # Calcular novo embedding
        embedding = self.embedding_model.embed_query(query)
        self.embedding_cache[cache_key] = embedding
        return embedding


# M√≥dulo de busca sem√¢ntica
class SemanticSearchService:
    """Servi√ßo para busca sem√¢ntica em documentos."""
    
    def __init__(self, embedding_service: EmbeddingService):
        """
        Inicializa o servi√ßo de busca sem√¢ntica.
        
        Args:
            embedding_service (EmbeddingService): Servi√ßo de embeddings a ser usado.
        """
        self.embedding_service = embedding_service
        # Cache para resultados de busca
        self.search_cache = {}
        # Tamanho m√°ximo do cache
        self.max_cache_size = 100
        # Contador de frequ√™ncia de consultas
        self.query_frequency = {}
        
    def _normalize_query(self, query: str) -> str:
        """
        Normaliza uma consulta para uso como chave de cache.
        
        Args:
            query (str): Consulta original.
            
        Returns:
            str: Consulta normalizada.
        """
        # Normalizar para lowercase e remover espa√ßos extras
        normalized = query.lower().strip()
        # Remover pontua√ß√£o comum
        for char in ".,;:!?":
            normalized = normalized.replace(char, "")
        return normalized
        
    def _get_cache_key(self, query: str, k: int) -> str:
        """
        Gera uma chave de cache para uma consulta e k.
        
        Args:
            query (str): Consulta normalizada.
            k (int): N√∫mero de resultados.
            
        Returns:
            str: Chave de cache.
        """
        return f"{query}_{k}"
    
    def _update_query_frequency(self, query: str):
        """
        Atualiza a frequ√™ncia de uma consulta.
        
        Args:
            query (str): Consulta normalizada.
        """
        if query in self.query_frequency:
            self.query_frequency[query] += 1
        else:
            self.query_frequency[query] = 1
    
    def _clean_cache_if_needed(self):
        """
        Limpa o cache se ele exceder o tamanho m√°ximo.
        """
        if len(self.search_cache) <= self.max_cache_size:
            return
            
        # Ordenar consultas por frequ√™ncia
        sorted_queries = sorted(
            self.query_frequency.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        # Manter apenas as consultas mais frequentes
        queries_to_keep = set([q for q, _ in sorted_queries[:self.max_cache_size // 2]])
        
        # Remover do cache as consultas menos frequentes
        new_cache = {}
        for cache_key, results in self.search_cache.items():
            query = cache_key.split("_")[0]
            if query in queries_to_keep:
                new_cache[cache_key] = results
        
        # Atualizar o cache e o contador de frequ√™ncia
        self.search_cache = new_cache
        self.query_frequency = {q: f for q, f in self.query_frequency.items() if q in queries_to_keep}
        
        logger.info(f"Cache cleaned. Kept {len(self.search_cache)} entries based on frequency.")
    
    def search(self, query: str, documents: List[Document], doc_embeddings: Dict[int, List[float]], k: int = 3):
        """
        Realiza busca sem√¢ntica em documentos.
        
        Args:
            query (str): Consulta para busca.
            documents (List[Document]): Lista de documentos para buscar.
            doc_embeddings (Dict[int, List[float]]): Dicion√°rio de embeddings por √≠ndice de documento.
            k (int): N√∫mero de resultados a retornar.
            
        Returns:
            List[Dict]: Lista de resultados com score, texto e metadados.
        """
        if not documents:
            return []
        
        # Normalizar query e gerar chave de cache
        normalized_query = self._normalize_query(query)
        cache_key = self._get_cache_key(normalized_query, k)
        
        # Atualizar frequ√™ncia
        self._update_query_frequency(normalized_query)
        
        # Verificar cache
        if cache_key in self.search_cache:
            logger.info(f"Cache hit for query: {query}")
            return self.search_cache[cache_key]
        
        # Generate query embedding
        query_embedding = self.embedding_service.embed_query(query)
        
        import numpy as np
        from sklearn.metrics.pairwise import cosine_similarity
        
        # Preparar lista de embeddings a partir do dicion√°rio
        doc_embedding_list = [doc_embeddings[i] for i in range(len(documents))]
        
        # Calculate similarity
        similarities = cosine_similarity([query_embedding], doc_embedding_list)[0]
        
        # Get top k results
        top_indices = np.argsort(similarities)[-k:][::-1]
        results = []
        
        for idx in top_indices:
            results.append({
                "score": float(similarities[idx]),
                "text": documents[idx].page_content,
                "metadata": documents[idx].metadata
            })
        
        # Armazenar no cache
        self.search_cache[cache_key] = results
        
        # Limpar cache se necess√°rio
        self._clean_cache_if_needed()
        
        return results


# M√≥dulo de gera√ß√£o de prompts
class PromptGenerator:
    """Gerador de prompts para o chatbot de vendas."""
    
    def __init__(self):
        """Inicializa o gerador de prompts."""
        # Varia√ß√µes de linguagem para tornar as respostas mais naturais
        self.greeting_variations = [
            "Ol√°! Sou o Davi, gerente comercial do JARVIS.",
            "Oi! Aqui √© o Davi, do time JARVIS.",
            "E a√≠! Davi do JARVIS aqui para te ajudar.",
            "Ol√°! Davi Cardoso, do JARVIS, prazer em conhec√™-lo!",
            "Oi! Davi, do time comercial do JARVIS. Como posso ajudar?"
        ]
        
        self.closing_variations = [
            "Estou √† disposi√ß√£o para tirar qualquer d√∫vida!",
            "Ficou alguma d√∫vida? Estou aqui para ajudar!",
            "O que voc√™ acha? Posso te ajudar com mais detalhes?",
            "Gostaria de saber mais sobre algum aspecto espec√≠fico?",
            "Estou aqui para o que precisar! O que achou?"
        ]
        
        self.urgency_phrases = [
            "As vagas para teste gratuito s√£o limitadas nesta semana!",
            "Estamos com uma promo√ß√£o especial at√© o final da semana.",
            "O pre√ßo atual √© promocional e vai aumentar em breve.",
            "As pr√≥ximas atualiza√ß√µes estar√£o dispon√≠veis apenas para usu√°rios j√° cadastrados.",
            "Temos poucas vagas restantes para o acesso antecipado.",
            "Esta √© uma chance √∫nica de garantir o valor atual.",
            "Muitos usu√°rios est√£o aderindo agora para garantir este pre√ßo.",
            "Isso significa menos de R$0,14 por dia - por tempo limitado!"
        ]
    
    def _select_random_variation(self, variations_list):
        """
        Seleciona uma varia√ß√£o aleat√≥ria de texto.
        
        Args:
            variations_list (list): Lista de varia√ß√µes.
            
        Returns:
            str: Varia√ß√£o selecionada.
        """
        import random
        return random.choice(variations_list)
    
    def _adapt_to_conversation_stage(self, conversation_history=None):
        """
        Adapta o prompt ao est√°gio da conversa.
        
        Args:
            conversation_history (list, optional): Hist√≥rico de conversa.
            
        Returns:
            dict: Configura√ß√µes adaptadas.
        """
        if not conversation_history:
            # Primeira mensagem - foco em apresenta√ß√£o e entendimento
            return {
                "include_greeting": True,
                "focus_on_understanding": True,
                "include_product_details": False,
                "include_pricing": False,
                "include_urgency": False,
                "include_link": False
            }
        
        msg_count = len(conversation_history) // 2  # Par de mensagens (user/assistant)
        
        if msg_count == 1:
            # Segunda intera√ß√£o - entender necessidades, mencionar benef√≠cios
            return {
                "include_greeting": False,
                "focus_on_understanding": True,
                "include_product_details": True,
                "include_pricing": False,
                "include_urgency": False,
                "include_link": False
            }
        elif msg_count == 2:
            # Terceira intera√ß√£o - detalhar solu√ß√£o, mencionar pre√ßo
            return {
                "include_greeting": False,
                "focus_on_understanding": False,
                "include_product_details": True,
                "include_pricing": True,
                "include_urgency": True,
                "include_link": True
            }
        else:
            # Intera√ß√µes posteriores - foco em convers√£o, urg√™ncia
            return {
                "include_greeting": False,
                "focus_on_understanding": False,
                "include_product_details": True,
                "include_pricing": True,
                "include_urgency": True,
                "include_link": True
            }
    
    def generate_system_prompt(self, conversation_history=None) -> str:
        """
        Gera o prompt do sistema para o chatbot de vendas, adaptado ao est√°gio da conversa.
        
        Args:
            conversation_history (list, optional): Hist√≥rico de conversa.
            
        Returns:
            str: Prompt do sistema.
        """
        # Adaptar ao est√°gio da conversa
        config = self._adapt_to_conversation_stage(conversation_history)
        
        # Selecionar varia√ß√µes para uso neste prompt
        greeting = self._select_random_variation(self.greeting_variations)
        closing = self._select_random_variation(self.closing_variations)
        urgency_phrase = self._select_random_variation(self.urgency_phrases)
        
        # Construir o prompt base
        prompt = """Voc√™ √© Davi Cardoso, Gerente Comercial do JARVIS. Voc√™ deve se comunicar no estilo do WhatsApp - mensagens curtas, diretas e com boa formata√ß√£o.

ESTILO DE COMUNICA√á√ÉO NO WHATSAPP:
- Use mensagens curtas e objetivas (2-4 par√°grafos no m√°ximo)
- Evite textos longos que parecem um manual de instru√ß√µes
- Use emojis com modera√ß√£o para humanizar a conversa 
- Utilize quebras de linha para melhorar a legibilidade
- Destaque pontos importantes com *asteriscos* para negrito
- Seja conversacional como em um bate-papo real
- Fa√ßa perguntas curtas e espec√≠ficas"""

        # Adicionar se√ß√£o espec√≠fica para o est√°gio atual
        if config["focus_on_understanding"]:
            prompt += """

NESTE MOMENTO DA CONVERSA:
- FOQUE EM ENTENDER A NECESSIDADE: Fa√ßa perguntas sobre os desafios de produtividade do cliente
- DEMONSTRE EMPATIA: Valide as dificuldades mencionadas pelo cliente
- EVITE FALAR MUITO DO PRODUTO: Apenas mencione benef√≠cios diretamente relacionados √†s necessidades expressas
- PERSONALIZE: Use o nome do cliente se ele o tiver mencionado"""
        else:
            prompt += """

NESTE MOMENTO DA CONVERSA:
- CONECTE NECESSIDADES COM SOLU√á√ïES: Mostre como o JARVIS resolve os problemas mencionados
- SEJA ESPEC√çFICO: Cite funcionalidades relevantes para o caso do cliente
- INCLUA HIST√ìRIAS DE SUCESSO: Mencione brevemente como outros usu√°rios resolveram problemas similares
- MANTENHA O DI√ÅLOGO: Continue fazendo perguntas para ajustar sua resposta"""

        # Adicionar o restante do prompt
        prompt += """

ABORDAGEM DE VENDAS:
- PRIMEIRO ENTENDA, DEPOIS OFERE√áA: Fa√ßa perguntas para entender a necessidade antes de falar sobre o produto
- MENOS √â MAIS: Forne√ßa apenas informa√ß√µes relevantes para o momento da conversa
- FOCO NO CLIENTE: Concentre-se nos problemas e necessidades espec√≠ficas do cliente
- LINGUAGEM SIMPLES: Evite jarg√µes t√©cnicos complexos
- GRADUALMENTE: Revele informa√ß√µes em pequenas doses, n√£o tudo de uma vez

COMPARTILHE O LINK:
- Sempre compartilhe o link da plataforma: https://www.appjarvis.com.br/
- Momentos ideais para compartilhar o link:
  1. Quando o cliente mostrar interesse em conhecer o produto
  2. Logo ap√≥s descrever um benef√≠cio que desperte curiosidade
  3. Quando o cliente perguntar como acessar ou se cadastrar
  4. No final da conversa como call-to-action
- Formate o link de forma destacada com emojis: üöÄ https://www.appjarvis.com.br/
- Refor√ße que o cadastro gratuito leva menos de 1 minuto

GATILHOS DE URG√äNCIA:
- Use a seguinte frase de urg√™ncia em algum momento da resposta: "{urgency_phrase}"
- Mencione que as vagas para o teste gratuito s√£o *limitadas* nesta semana
- Destaque que o pre√ßo atual de R$50/ano √© promocional e pode aumentar em breve
- Crie senso de oportunidade: "Esta √© uma chance √∫nica de..."

SEQU√äNCIA DE INTERA√á√ÉO:
1. Apresenta√ß√£o breve e amig√°vel (apenas na primeira mensagem)
2. Pergunte sobre necessidades/desafios espec√≠ficos
3. Escute atentamente antes de propor solu√ß√µes
4. Relacione as caracter√≠sticas do JARVIS com os problemas mencionados
5. Ofere√ßa valor antes de falar de pre√ßo
6. Use call-to-action simples e direto
7. Incorpore um elemento de urg√™ncia sutil e relevante ao final
8. Compartilhe o link quando o cliente mostrar interesse

SOBRE INFORMA√á√ïES DO PRODUTO:
- Mencione apenas o que √© relevante para a conversa atual
- Aprofunde detalhes somente quando solicitado
- Destaque benef√≠cios, n√£o recursos t√©cnicos
- Teste gratuito: 7 dias sem cart√£o de cr√©dito (*vagas limitadas*)
- Plano anual: R$50/ano (menos de R$4,20/m√™s) - *oferta por tempo limitado*
- Destaque que o JARVIS est√° em fase de crescimento e os pre√ßos tendem a aumentar
- Link do site: https://www.appjarvis.com.br/

IMPORTANTE: Quando o cliente fizer uma pergunta, responda diretamente e de forma concisa. N√£o transforme cada resposta em uma apresenta√ß√£o completa do produto. Construa a conversa gradualmente, como um di√°logo natural de WhatsApp. Insira elementos de urg√™ncia de forma natural e relevante ao contexto, evitando parecer agressivo ou desesperado."""

        # Adicionar elementos de personaliza√ß√£o
        if config["include_greeting"]:
            prompt += f"""

PARA ESTA MENSAGEM:
- Use a seguinte sauda√ß√£o (adaptando conforme necess√°rio): "{greeting}"
- Termine com: "{closing}"
- {'Inclua esta frase de urg√™ncia em algum momento: "' + urgency_phrase + '"' if config["include_urgency"] else ''}
- {'Compartilhe o link do site em algum momento da conversa.' if config["include_link"] else ''}"""

        return prompt
    
    def generate_augmented_prompt(self, query: str, sales_context: str, conversation_history=None) -> str:
        """
        Gera o prompt aumentado com contexto para o chatbot de vendas.
        
        Args:
            query (str): Consulta do usu√°rio.
            sales_context (str): Contexto de vendas relevante.
            conversation_history (list, optional): Hist√≥rico de conversa.
            
        Returns:
            str: Prompt aumentado.
        """
        # Adaptar ao est√°gio da conversa
        config = self._adapt_to_conversation_stage(conversation_history)
        
        # Extrair nome do cliente do hist√≥rico, se dispon√≠vel
        client_name = ""
        if conversation_history and len(conversation_history) > 0:
            for message in conversation_history:
                if message["role"] == "user" and "meu nome √©" in message["content"].lower():
                    name_parts = message["content"].lower().split("meu nome √©")[1].strip().split()
                    if name_parts:
                        client_name = name_parts[0].capitalize()
                        break
        
        # Construir instru√ß√µes espec√≠ficas
        specific_instructions = []
        
        if config["focus_on_understanding"]:
            specific_instructions.append("- Fa√ßa perguntas para entender melhor as necessidades espec√≠ficas do cliente")
            specific_instructions.append("- Evite listar muitas funcionalidades do produto")
        
        if config["include_product_details"]:
            specific_instructions.append("- Mencione apenas os benef√≠cios do JARVIS que s√£o relevantes para as necessidades expressas")
        
        if config["include_pricing"]:
            specific_instructions.append("- Mencione o per√≠odo de teste gratuito e o valor promocional")
        
        if config["include_urgency"]:
            specific_instructions.append("- Inclua um elemento de urg√™ncia para incentivar a√ß√£o imediata")
        
        if config["include_link"]:
            specific_instructions.append("- Compartilhe o link do site: https://www.appjarvis.com.br/")
        
        if client_name:
            specific_instructions.append(f"- Personalize a resposta usando o nome do cliente: {client_name}")
        
        # Juntar instru√ß√µes
        instructions_text = "\n".join(specific_instructions)
        
        prompt = f"""Use o contexto abaixo sobre o JARVIS para responder √† pergunta do potencial cliente
de forma persuasiva mas conversacional, como uma conversa de WhatsApp. Seja conciso e direto.

Instru√ß√µes espec√≠ficas para esta resposta:
{instructions_text}

Contexto sobre o JARVIS:
{sales_context}

Pergunta/obje√ß√£o do cliente: {query}"""

        return prompt
    
    def generate_sales_prompt(self, query: str, sales_results: List[Dict], conversation_history=None) -> tuple:
        """
        Gera o prompt do sistema e o prompt aumentado para o chatbot de vendas.
        
        Args:
            query (str): Consulta do usu√°rio.
            sales_results (List[Dict]): Resultados da busca sem√¢ntica.
            conversation_history (list, optional): Hist√≥rico de conversa.
            
        Returns:
            tuple: (prompt do sistema, prompt aumentado)
        """
        # Extract text from results
        sales_context = "\n".join([result["text"] for result in sales_results])
        
        # Generate prompts
        system_prompt = self.generate_system_prompt(conversation_history)
        augmented_prompt = self.generate_augmented_prompt(query, sales_context, conversation_history)
        
        return system_prompt, augmented_prompt


# M√≥dulo de LLM
class LLMService:
    """Servi√ßo para intera√ß√£o com o modelo de linguagem grande."""
    
    def __init__(self, llm_client=None, timeout=30, max_retries=2):
        """
        Inicializa o servi√ßo de LLM.
        
        Args:
            llm_client: Cliente para o modelo de linguagem grande. Se None, usa Groq padr√£o.
            timeout (int): Tempo limite para requisi√ß√µes em segundos.
            max_retries (int): N√∫mero m√°ximo de tentativas em caso de falha.
        """
        self.llm_client = llm_client or Groq(api_key=os.getenv("GROQ_API_KEY"))
        self.timeout = timeout
        self.max_retries = max_retries
        self.fallback_responses = {
            "error_timeout": "Desculpe, o tempo de resposta excedeu o limite. Poderia reformular sua pergunta de forma mais espec√≠fica?",
            "error_api": "Estamos enfrentando dificuldades t√©cnicas moment√¢neas. Por favor, tente novamente em alguns instantes.",
            "error_unknown": "Ocorreu um erro inesperado. Nossa equipe foi notificada e est√° trabalhando para resolver."
        }
        
    def convert_messages_to_groq_format(self, messages):
        """
        Converte mensagens do formato LangChain para o formato Groq.
        
        Args:
            messages: Lista de mensagens no formato LangChain.
            
        Returns:
            List[Dict]: Lista de mensagens no formato Groq.
        """
        groq_messages = []
        for message in messages:
            if isinstance(message, SystemMessage):
                groq_messages.append({"role": "system", "content": message.content})
            elif isinstance(message, HumanMessage):
                groq_messages.append({"role": "user", "content": message.content})
            elif isinstance(message, AIMessage):
                groq_messages.append({"role": "assistant", "content": message.content})
        return groq_messages
    
    def _log_error_details(self, error, context=None):
        """
        Registra detalhes de erro para diagn√≥stico.
        
        Args:
            error (Exception): Erro ocorrido.
            context (dict, optional): Contexto adicional sobre o erro.
        """
        error_type = type(error).__name__
        error_msg = str(error)
        
        error_details = {
            "error_type": error_type,
            "error_message": error_msg,
            "timestamp": datetime.now().isoformat()
        }
        
        if context:
            error_details["context"] = context
            
        logger.error(f"LLM Error: {error_type} - {error_msg}", extra={"error_details": error_details})
        
    def _execute_with_retries(self, func, *args, **kwargs):
        """
        Executa uma fun√ß√£o com retentativas.
        
        Args:
            func: Fun√ß√£o a ser executada.
            *args, **kwargs: Argumentos para a fun√ß√£o.
            
        Returns:
            Resultado da fun√ß√£o.
            
        Raises:
            Exception: Se todas as tentativas falharem.
        """
        import time
        
        retries = 0
        last_error = None
        
        while retries <= self.max_retries:
            try:
                if retries > 0:
                    # Espera exponencial entre tentativas
                    time.sleep(2 ** retries)
                    logger.info(f"Retry {retries}/{self.max_retries} for LLM request")
                
                # Executar a fun√ß√£o diretamente (sem async)
                return func(*args, **kwargs)
                    
            except Exception as e:
                last_error = e
                retries += 1
                self._log_error_details(e, {"retry_count": retries})
        
        # Se chegou aqui, todas as tentativas falharam
        error_msg = f"All {self.max_retries + 1} attempts failed"
        logger.error(error_msg, exc_info=last_error)
        raise last_error
        
    def generate_response(self, messages, temperature=0.7, max_tokens=1024):
        """
        Gera uma resposta a partir de mensagens.
        
        Args:
            messages: Lista de mensagens no formato LangChain.
            temperature (float): Temperatura para gera√ß√£o.
            max_tokens (int): N√∫mero m√°ximo de tokens.
            
        Returns:
            str: Resposta gerada.
        """
        try:
            # Convert to Groq format
            groq_messages = self.convert_messages_to_groq_format(messages)
            
            # Define a fun√ß√£o para invocar o modelo
            def invoke_model():
                return self.llm_client.chat.completions.create(
                    model=config.GROQ_MODEL_NAME,
                    messages=groq_messages,
                    temperature=temperature,
                    max_completion_tokens=max_tokens,
                    top_p=0.95,
                    stream=False,
                    reasoning_format="hidden"
                )
            
            # Execute com timeout e retries
            try:
                # Executar com timeout
                import threading
                import queue
                
                # Usar uma fila para obter o resultado ou exce√ß√£o
                result_queue = queue.Queue()
                
                def execute_with_queue():
                    try:
                        result = self._execute_with_retries(invoke_model)
                        result_queue.put(("success", result))
                    except Exception as e:
                        result_queue.put(("error", e))
                
                # Criar thread para executar a chamada
                thread = threading.Thread(target=execute_with_queue)
                thread.daemon = True
                thread.start()
                
                # Esperar o resultado com timeout
                try:
                    status, result = result_queue.get(timeout=self.timeout)
                    if status == "error":
                        raise result
                    completion = result
                    return completion.choices[0].message.content
                except queue.Empty:
                    logger.error(f"LLM request timed out after {self.timeout}s")
                    return self.fallback_responses["error_timeout"]
                
            except Exception as e:
                error_type = type(e).__name__
                self._log_error_details(e, {"error_type": "timeout_handler"})
                if "timeout" in error_type.lower():
                    return self.fallback_responses["error_timeout"]
                else:
                    raise e
                
        except Exception as e:
            error_type = type(e).__name__
            self._log_error_details(e, {
                "messages_count": len(messages),
                "temperature": temperature,
                "max_tokens": max_tokens
            })
            
            # Fornecer respostas de fallback espec√≠ficas
            if "timeout" in error_type.lower():
                return self.fallback_responses["error_timeout"]
            elif any(term in str(e).lower() for term in ["api", "key", "auth", "credential"]):
                return self.fallback_responses["error_api"]
            else:
                return f"{self.fallback_responses['error_unknown']} ({error_type})"


# Classe principal do servi√ßo RAG
class SalesRAGService:
    """Servi√ßo de RAG para conte√∫do de vendas do JARVIS."""
    
    def __init__(self, embedding_model=None, llm_client=None):
        """
        Inicializa o servi√ßo de RAG para vendas.
        
        Args:
            embedding_model: Modelo de embeddings a usar.
            llm_client: Cliente LLM a usar.
        """
        # Iniciar componentes
        self.content_manager = ContentManager(JARVIS_SALES_CONTENT)
        self.embedding_service = EmbeddingService(embedding_model)
        self.search_service = SemanticSearchService(self.embedding_service)
        self.prompt_generator = PromptGenerator()
        self.llm_service = LLMService(llm_client)
        
        # Configura√ß√µes
        self.top_k_results = 3  # N√∫mero padr√£o de resultados a retornar
        self.debug_mode = False  # Modo de debug desativado por padr√£o
        
        # Carregar conte√∫do de vendas
        self.load_sales_content()
        
        logger.info(f"Sales RAG Service initialized with model: {config.GROQ_MODEL_NAME}")
    
    def load_sales_content(self):
        """Carrega conte√∫do de vendas e gera embeddings."""
        try:
            # Carregar e processar conte√∫do
            self.sales_docs = self.content_manager.load_content()
            
            # Gerar embeddings para todos os documentos
            self.doc_embeddings = self.embedding_service.get_embeddings_for_documents(self.sales_docs)
            
            logger.info(f"Loaded {len(self.sales_docs)} sales content chunks with embeddings")
            return {"status": "success", "message": f"Loaded {len(self.sales_docs)} sales content chunks"}
        except Exception as e:
            logger.error(f"Error loading sales content: {str(e)}", exc_info=True)
            # Inicializar com conte√∫do padr√£o para evitar erros em cascata
            self.sales_docs = [
                Document(
                    page_content="JARVIS: Seu Assistente Pessoal de IA. O JARVIS √© um assistente pessoal de produtividade alimentado por intelig√™ncia artificial.",
                    metadata={"section": "Geral", "chunk_id": 0, "source": "default_content"}
                )
            ]
            self.doc_embeddings = {0: self.embedding_service.embed_query("JARVIS: Seu Assistente Pessoal de IA")}
            return {"status": "error", "message": f"Error loading sales content: {str(e)}"}
    
    def search_sales_content(self, query: str, k: int = 3):
        """
        Busca conte√∫do de vendas relevante com base na consulta.
        
        Args:
            query (str): Consulta do usu√°rio.
            k (int): N√∫mero de resultados a retornar.
            
        Returns:
            List[Dict]: Lista de resultados relevantes.
        """
        return self.search_service.search(query, self.sales_docs, self.doc_embeddings, k)
    
    def generate_sales_prompt(self, query: str, k: int = 3, conversation_history=None):
        """
        Gera prompt de vendas persuasivo com base na consulta.
        
        Args:
            query (str): Consulta do usu√°rio.
            k (int): N√∫mero de documentos de contexto a usar.
            conversation_history (list, optional): Hist√≥rico da conversa.
            
        Returns:
            tuple: (prompt do sistema, prompt aumentado)
        """
        # Buscar conte√∫do relevante
        sales_results = self.search_sales_content(query, k=k)
        
        # Gerar prompts
        return self.prompt_generator.generate_sales_prompt(query, sales_results, conversation_history)
    
    def answer_sales_query(self, query, conversation_history=None):
        """
        Processa uma pergunta de vendas e gera uma resposta.
        
        Args:
            query (str): Pergunta do usu√°rio.
            conversation_history (list, optional): Hist√≥rico de conversa√ß√£o.
            
        Returns:
            dict: Resposta formatada.
        """
        try:
            # Normalizando a query e dados
            query = query.strip() if query else ""
            conversation_history = conversation_history or []
            
            if not query:
                return {"response": "Parece que voc√™ n√£o enviou uma mensagem. Como posso ajudar?"}
            
            # Obtendo os dados relevantes atrav√©s de pesquisa sem√¢ntica
            search_results = self.search_service.search(
                query=query,
                documents=self.sales_docs,
                doc_embeddings=self.doc_embeddings,
                k=self.top_k_results
            )
            
            if not search_results:
                logger.warning(f"No search results found for query: {query}")
            
            # Detectando o nome do cliente (opcional)
            client_name = ""
            if conversation_history and len(conversation_history) > 0:
                for message in conversation_history:
                    if message["role"] == "user" and "meu nome √©" in message["content"].lower():
                        name_parts = message["content"].lower().split("meu nome √©")[1].strip().split()
                        if name_parts:
                            client_name = name_parts[0].capitalize()
                            break
            
            # Gerando o prompt do sistema e o prompt aumentado
            system_prompt, augmented_prompt = self.prompt_generator.generate_sales_prompt(
                query=query,
                sales_results=search_results,
                conversation_history=conversation_history
            )
            
            # Inicializar mensagens com prompt do sistema
            from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
            
            messages = [SystemMessage(content=system_prompt)]
            
            # Adicionar hist√≥rico de conversa
            if conversation_history:
                for entry in conversation_history:
                    if entry["role"] == "user":
                        messages.append(HumanMessage(content=entry["content"]))
                    elif entry["role"] == "assistant":
                        messages.append(AIMessage(content=entry["content"]))
            
            # Adicionar consulta atual com contexto
            messages.append(HumanMessage(content=augmented_prompt))
            
            # Gerando a resposta final usando o modelo de linguagem
            response_content = self.llm_service.generate_response(
                messages=messages,
                temperature=0.8,
                max_tokens=1500
            )
            
            return {
                "response": response_content,
                "search_results": search_results if self.debug_mode else None
            }
            
        except Exception as e:
            logger.error(f"Error answering sales query: {str(e)}", exc_info=True)
            return {
                "response": "Desculpe, estou enfrentando algumas dificuldades t√©cnicas no momento. Poderia tentar novamente em alguns instantes?",
                "error": str(e) if self.debug_mode else None
            } 